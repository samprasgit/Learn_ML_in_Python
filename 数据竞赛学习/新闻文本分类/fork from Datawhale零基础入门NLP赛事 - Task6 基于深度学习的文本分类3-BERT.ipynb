{"cells":[{"cell_type":"markdown","source":"# BERT\n\n微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。  \n\n预训练BERT以及相关代码下载地址：链接: https://pan.baidu.com/s/1zd6wN7elGgp1NyuzYKpvGQ 提取码: tmp5\n\n![BERT](img/bert.png)","metadata":{}},{"cell_type":"code","source":"import logging\nimport random\n\nimport numpy as np\nimport torch\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n\n# set seed\nseed = 666\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\n\n# set cuda\ngpu = 0\nuse_cuda = gpu >= 0 and torch.cuda.is_available()\nif use_cuda:\n    torch.cuda.set_device(gpu)\n    device = torch.device(\"cuda\", gpu)\nelse:\n    device = torch.device(\"cpu\")\nlogging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)","metadata":{},"execution_count":1,"outputs":[{"name":"stderr","output_type":"stream","text":"2020-07-17 12:02:34,773 INFO: Use cuda: True, gpu id: 0.\n"}]},{"cell_type":"code","source":"# split data to 10 fold\nfold_num = 10\ndata_file = '../data/train_set.csv'\nimport pandas as pd\n\n\ndef all_data2fold(fold_num, num=10000):\n    fold_data = []\n    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n    texts = f['text'].tolist()[:num]\n    labels = f['label'].tolist()[:num]\n\n    total = len(labels)\n\n    index = list(range(total))\n    np.random.shuffle(index)\n\n    all_texts = []\n    all_labels = []\n    for i in index:\n        all_texts.append(texts[i])\n        all_labels.append(labels[i])\n\n    label2id = {}\n    for i in range(total):\n        label = str(all_labels[i])\n        if label not in label2id:\n            label2id[label] = [i]\n        else:\n            label2id[label].append(i)\n\n    all_index = [[] for _ in range(fold_num)]\n    for label, data in label2id.items():\n        # print(label, len(data))\n        batch_size = int(len(data) / fold_num)\n        other = len(data) - batch_size * fold_num\n        for i in range(fold_num):\n            cur_batch_size = batch_size + 1 if i < other else batch_size\n            # print(cur_batch_size)\n            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n            all_index[i].extend(batch_data)\n\n    batch_size = int(total / fold_num)\n    other_texts = []\n    other_labels = []\n    other_num = 0\n    start = 0\n    for fold in range(fold_num):\n        num = len(all_index[fold])\n        texts = [all_texts[i] for i in all_index[fold]]\n        labels = [all_labels[i] for i in all_index[fold]]\n\n        if num > batch_size:\n            fold_texts = texts[:batch_size]\n            other_texts.extend(texts[batch_size:])\n            fold_labels = labels[:batch_size]\n            other_labels.extend(labels[batch_size:])\n            other_num += num - batch_size\n        elif num < batch_size:\n            end = start + batch_size - num\n            fold_texts = texts + other_texts[start: end]\n            fold_labels = labels + other_labels[start: end]\n            start = end\n        else:\n            fold_texts = texts\n            fold_labels = labels\n\n        assert batch_size == len(fold_labels)\n\n        # shuffle\n        index = list(range(batch_size))\n        np.random.shuffle(index)\n\n        shuffle_fold_texts = []\n        shuffle_fold_labels = []\n        for i in index:\n            shuffle_fold_texts.append(fold_texts[i])\n            shuffle_fold_labels.append(fold_labels[i])\n\n        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n        fold_data.append(data)\n\n    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n\n    return fold_data\n\n\nfold_data = all_data2fold(10)","metadata":{},"execution_count":2,"outputs":[{"name":"stderr","output_type":"stream","text":"2020-07-17 12:02:39,180 INFO: Fold lens [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"}]},{"cell_type":"code","source":"# build train, dev, test data\nfold_id = 9\n\n# dev\ndev_data = fold_data[fold_id]\n\n# train\ntrain_texts = []\ntrain_labels = []\nfor i in range(0, fold_id):\n    data = fold_data[i]\n    train_texts.extend(data['text'])\n    train_labels.extend(data['label'])\n\ntrain_data = {'label': train_labels, 'text': train_texts}\n\n# test\ntest_data_file = '../data/test_a.csv'\nf = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8')\ntexts = f['text'].tolist()\ntest_data = {'label': [0] * len(texts), 'text': texts}","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# build vocab\nfrom collections import Counter\nfrom transformers import BasicTokenizer\n\nbasic_tokenizer = BasicTokenizer()\n\n\nclass Vocab():\n    def __init__(self, train_data):\n        self.min_count = 5\n        self.pad = 0\n        self.unk = 1\n        self._id2word = ['[PAD]', '[UNK]']\n        self._id2extword = ['[PAD]', '[UNK]']\n\n        self._id2label = []\n        self.target_names = []\n\n        self.build_vocab(train_data)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        self._word2id = reverse(self._id2word)\n        self._label2id = reverse(self._id2label)\n\n        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n\n    def build_vocab(self, data):\n        self.word_counter = Counter()\n\n        for text in data['text']:\n            words = text.split()\n            for word in words:\n                self.word_counter[word] += 1\n\n        for word, count in self.word_counter.most_common():\n            if count >= self.min_count:\n                self._id2word.append(word)\n\n        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n\n        self.label_counter = Counter(data['label'])\n\n        for label in range(len(self.label_counter)):\n            count = self.label_counter[label]\n            self._id2label.append(label)\n            self.target_names.append(label2name[label])\n\n    def load_pretrained_embs(self, embfile):\n        with open(embfile, encoding='utf-8') as f:\n            lines = f.readlines()\n            items = lines[0].split()\n            word_count, embedding_dim = int(items[0]), int(items[1])\n\n        index = len(self._id2extword)\n        embeddings = np.zeros((word_count + index, embedding_dim))\n        for line in lines[1:]:\n            values = line.split()\n            self._id2extword.append(values[0])\n            vector = np.array(values[1:], dtype='float64')\n            embeddings[self.unk] += vector\n            embeddings[index] = vector\n            index += 1\n\n        embeddings[self.unk] = embeddings[self.unk] / word_count\n        embeddings = embeddings / np.std(embeddings)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        self._extword2id = reverse(self._id2extword)\n\n        assert len(set(self._id2extword)) == len(self._id2extword)\n\n        return embeddings\n\n    def word2id(self, xs):\n        if isinstance(xs, list):\n            return [self._word2id.get(x, self.unk) for x in xs]\n        return self._word2id.get(xs, self.unk)\n\n    def extword2id(self, xs):\n        if isinstance(xs, list):\n            return [self._extword2id.get(x, self.unk) for x in xs]\n        return self._extword2id.get(xs, self.unk)\n\n    def label2id(self, xs):\n        if isinstance(xs, list):\n            return [self._label2id.get(x, self.unk) for x in xs]\n        return self._label2id.get(xs, self.unk)\n\n    @property\n    def word_size(self):\n        return len(self._id2word)\n\n    @property\n    def extword_size(self):\n        return len(self._id2extword)\n\n    @property\n    def label_size(self):\n        return len(self._id2label)\n\n\nvocab = Vocab(train_data)","metadata":{},"execution_count":4,"outputs":[{"name":"stderr","output_type":"stream","text":"2020-07-17 12:02:40,225 INFO: PyTorch version 1.2.0 available.\n\n2020-07-17 12:02:43,280 INFO: Build vocab: words 4337, labels 14.\n"}]},{"cell_type":"code","source":"# build module\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        self.weight.data.normal_(mean=0.0, std=0.05)\n\n        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n        b = np.zeros(hidden_size, dtype=np.float32)\n        self.bias.data.copy_(torch.from_numpy(b))\n\n        self.query = nn.Parameter(torch.Tensor(hidden_size))\n        self.query.data.normal_(mean=0.0, std=0.05)\n\n    def forward(self, batch_hidden, batch_masks):\n        # batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)\n        # batch_masks:  b x len\n\n        # linear\n        key = torch.matmul(batch_hidden, self.weight) + self.bias  # b x len x hidden\n\n        # compute attention\n        outputs = torch.matmul(key, self.query)  # b x len\n\n        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n\n        attn_scores = F.softmax(masked_outputs, dim=1)  # b x len\n\n        # 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0\n        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n\n        # sum weighted sources\n        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)  # b x hidden\n\n        return batch_outputs, attn_scores\n\n\n# build word encoder\nbert_path = '../emb/bert-mini/'\ndropout = 0.15\n\nfrom transformers import BertModel\n\n\nclass WordBertEncoder(nn.Module):\n    def __init__(self):\n        super(WordBertEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        self.tokenizer = WhitespaceTokenizer()\n        self.bert = BertModel.from_pretrained(bert_path)\n\n        self.pooled = False\n        logging.info('Build Bert encoder with pooled {}.'.format(self.pooled))\n\n    def encode(self, tokens):\n        tokens = self.tokenizer.tokenize(tokens)\n        return tokens\n\n    def get_bert_parameters(self):\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_parameters = [\n            {'params': [p for n, p in self.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n             'weight_decay': 0.01},\n            {'params': [p for n, p in self.bert.named_parameters() if any(nd in n for nd in no_decay)],\n             'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    def forward(self, input_ids, token_type_ids):\n        # input_ids: sen_num x bert_len\n        # token_type_ids: sen_num  x bert_len\n\n        # sen_num x bert_len x 256, sen_num x 256\n        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n\n        if self.pooled:\n            reps = pooled_output\n        else:\n            reps = sequence_output[:, 0, :]  # sen_num x 256\n\n        if self.training:\n            reps = self.dropout(reps)\n\n        return reps\n\n\nclass WhitespaceTokenizer():\n    \"\"\"WhitespaceTokenizer with vocab.\"\"\"\n\n    def __init__(self):\n        vocab_file = bert_path + 'vocab.txt'\n        self._token2id = self.load_vocab(vocab_file)\n        self._id2token = {v: k for k, v in self._token2id.items()}\n        self.max_len = 256\n        self.unk = 1\n\n        logging.info(\"Build Bert vocab with size %d.\" % (self.vocab_size))\n\n    def load_vocab(self, vocab_file):\n        f = open(vocab_file, 'r')\n        lines = f.readlines()\n        lines = list(map(lambda x: x.strip(), lines))\n        vocab = dict(zip(lines, range(len(lines))))\n        return vocab\n\n    def tokenize(self, tokens):\n        assert len(tokens) <= self.max_len - 2\n        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n        output_tokens = self.token2id(tokens)\n        return output_tokens\n\n    def token2id(self, xs):\n        if isinstance(xs, list):\n            return [self._token2id.get(x, self.unk) for x in xs]\n        return self._token2id.get(xs, self.unk)\n\n    @property\n    def vocab_size(self):\n        return len(self._id2token)\n\n\n# build sent encoder\nsent_hidden_size = 256\nsent_num_layers = 2\n\n\nclass SentEncoder(nn.Module):\n    def __init__(self, sent_rep_size):\n        super(SentEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        self.sent_lstm = nn.LSTM(\n            input_size=sent_rep_size,\n            hidden_size=sent_hidden_size,\n            num_layers=sent_num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n\n    def forward(self, sent_reps, sent_masks):\n        # sent_reps:  b x doc_len x sent_rep_size\n        # sent_masks: b x doc_len\n\n        sent_hiddens, _ = self.sent_lstm(sent_reps)  # b x doc_len x hidden*2\n        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n\n        if self.training:\n            sent_hiddens = self.dropout(sent_hiddens)\n\n        return sent_hiddens","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# build model\nclass Model(nn.Module):\n    def __init__(self, vocab):\n        super(Model, self).__init__()\n        self.sent_rep_size = 256\n        self.doc_rep_size = sent_hidden_size * 2\n        self.all_parameters = {}\n        parameters = []\n        self.word_encoder = WordBertEncoder()\n        bert_parameters = self.word_encoder.get_bert_parameters()\n\n        self.sent_encoder = SentEncoder(self.sent_rep_size)\n        self.sent_attention = Attention(self.doc_rep_size)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n\n        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n\n        if use_cuda:\n            self.to(device)\n\n        if len(parameters) > 0:\n            self.all_parameters[\"basic_parameters\"] = parameters\n        self.all_parameters[\"bert_parameters\"] = bert_parameters\n\n        logging.info('Build model with bert word encoder, lstm sent encoder.')\n\n        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n        logging.info('Model param num: %.2f M.' % (para_num / 1e6))\n\n    def forward(self, batch_inputs):\n        # batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len\n        # batch_masks : b x doc_len x sent_len\n        batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n\n        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)  # sen_num x sent_rep_size\n\n        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  # b x doc_len x sent_rep_size\n        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  # b x doc_len x max_sent_len\n        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n\n        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  # b x doc_len x doc_rep_size\n        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  # b x doc_rep_size\n\n        batch_outputs = self.out(doc_reps)  # b x num_labels\n\n        return batch_outputs\n    \nmodel = Model(vocab)","metadata":{},"execution_count":6,"outputs":[{"name":"stderr","output_type":"stream","text":"2020-07-17 12:02:43,364 INFO: Build Bert vocab with size 5981.\n\n2020-07-17 12:02:43,365 INFO: loading configuration file ../emb/bert-mini/config.json\n\n2020-07-17 12:02:43,365 INFO: Model config BertConfig {\n\n  \"attention_probs_dropout_prob\": 0.1,\n\n  \"hidden_act\": \"gelu\",\n\n  \"hidden_dropout_prob\": 0.1,\n\n  \"hidden_size\": 256,\n\n  \"initializer_range\": 0.02,\n\n  \"intermediate_size\": 1024,\n\n  \"layer_norm_eps\": 1e-12,\n\n  \"max_position_embeddings\": 256,\n\n  \"model_type\": \"bert\",\n\n  \"num_attention_heads\": 4,\n\n  \"num_hidden_layers\": 4,\n\n  \"pad_token_id\": 0,\n\n  \"type_vocab_size\": 2,\n\n  \"vocab_size\": 5981\n\n}\n\n\n\n2020-07-17 12:02:43,366 INFO: loading weights file ../emb/bert-mini/pytorch_model.bin\n\n2020-07-17 12:02:43,439 INFO: Build Bert encoder with pooled False.\n\n2020-07-17 12:02:45,040 INFO: Build model with bert word encoder, lstm sent encoder.\n\n2020-07-17 12:02:45,041 INFO: Model param num: 7.72 M.\n"}]},{"cell_type":"code","source":"# build optimizer\nlearning_rate = 2e-4\nbert_lr = 5e-5\ndecay = .75\ndecay_step = 1000\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n\nclass Optimizer:\n    def __init__(self, model_parameters, steps):\n        self.all_params = []\n        self.optims = []\n        self.schedulers = []\n\n        for name, parameters in model_parameters.items():\n            if name.startswith(\"basic\"):\n                optim = torch.optim.Adam(parameters, lr=learning_rate)\n                self.optims.append(optim)\n\n                l = lambda step: decay ** (step // decay_step)\n                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n                self.schedulers.append(scheduler)\n                self.all_params.extend(parameters)\n            elif name.startswith(\"bert\"):\n                optim_bert = AdamW(parameters, bert_lr, eps=1e-8)\n                self.optims.append(optim_bert)\n\n                scheduler_bert = get_linear_schedule_with_warmup(optim_bert, 0, steps)\n                self.schedulers.append(scheduler_bert)\n\n                for group in parameters:\n                    for p in group['params']:\n                        self.all_params.append(p)\n            else:\n                Exception(\"no nameed parameters.\")\n\n        self.num = len(self.optims)\n\n    def step(self):\n        for optim, scheduler in zip(self.optims, self.schedulers):\n            optim.step()\n            scheduler.step()\n            optim.zero_grad()\n\n    def zero_grad(self):\n        for optim in self.optims:\n            optim.zero_grad()\n\n    def get_lr(self):\n        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n        lr = ' %.5f' * self.num\n        res = lr % lrs\n        return res","metadata":{},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# build dataset\ndef sentence_split(text, vocab, max_sent_len=256, max_segment=16):\n    words = text.strip().split()\n    document_len = len(words)\n\n    index = list(range(0, document_len, max_sent_len))\n    index.append(document_len)\n\n    segments = []\n    for i in range(len(index) - 1):\n        segment = words[index[i]: index[i + 1]]\n        assert len(segment) > 0\n        segment = [word if word in vocab._id2word else '<UNK>' for word in segment]\n        segments.append([len(segment), segment])\n\n    assert len(segments) > 0\n    if len(segments) > max_segment:\n        segment_ = int(max_segment / 2)\n        return segments[:segment_] + segments[-segment_:]\n    else:\n        return segments\n\n\ndef get_examples(data, word_encoder, vocab, max_sent_len=256, max_segment=8):\n    label2id = vocab.label2id\n    examples = []\n\n    for text, label in zip(data['text'], data['label']):\n        # label\n        id = label2id(label)\n\n        # words\n        sents_words = sentence_split(text, vocab, max_sent_len-2, max_segment)\n        doc = []\n        for sent_len, sent_words in sents_words:\n            token_ids = word_encoder.encode(sent_words)\n            sent_len = len(token_ids)\n            token_type_ids = [0] * sent_len\n            doc.append([sent_len, token_ids, token_type_ids])\n        examples.append([id, len(doc), doc])\n\n    logging.info('Total %d docs.' % len(examples))\n    return examples","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# build loader\n\ndef batch_slice(data, batch_size):\n    batch_num = int(np.ceil(len(data) / float(batch_size)))\n    for i in range(batch_num):\n        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n        docs = [data[i * batch_size + b] for b in range(cur_batch_size)]\n\n        yield docs\n\n\ndef data_iter(data, batch_size, shuffle=True, noise=1.0):\n    \"\"\"\n    randomly permute data, then sort by source length, and partition into batches\n    ensure that the length of  sentences in each batch\n    \"\"\"\n\n    batched_data = []\n    if shuffle:\n        np.random.shuffle(data)\n\n        lengths = [example[1] for example in data]\n        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) for l in lengths]\n        sorted_indices = np.argsort(noisy_lengths).tolist()\n        sorted_data = [data[i] for i in sorted_indices]\n    else:\n        sorted_data =data\n        \n    batched_data.extend(list(batch_slice(sorted_data, batch_size)))\n\n    if shuffle:\n        np.random.shuffle(batched_data)\n\n    for batch in batched_data:\n        yield batch","metadata":{},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# some function\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n\ndef get_score(y_ture, y_pred):\n    y_ture = np.array(y_ture)\n    y_pred = np.array(y_pred)\n    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n    p = precision_score(y_ture, y_pred, average='macro') * 100\n    r = recall_score(y_ture, y_pred, average='macro') * 100\n\n    return str((reformat(p, 2), reformat(r, 2), reformat(f1, 2))), reformat(f1, 2)\n\n\ndef reformat(num, n):\n    return float(format(num, '0.' + str(n) + 'f'))","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# build trainer\n\nimport time\nfrom sklearn.metrics import classification_report\n\nclip = 5.0\nepochs = 1\nearly_stops = 3\nlog_interval = 50\n\ntest_batch_size = 16\ntrain_batch_size = 16\n\nsave_model = './bert.bin'\nsave_test = './bert.csv'\n\nclass Trainer():\n    def __init__(self, model, vocab):\n        self.model = model\n        self.report = True\n        \n        self.train_data = get_examples(train_data, model.word_encoder, vocab)\n        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n        self.dev_data = get_examples(dev_data, model.word_encoder, vocab)\n        self.test_data = get_examples(test_data, model.word_encoder, vocab)\n\n        # criterion\n        self.criterion = nn.CrossEntropyLoss()\n\n        # label name\n        self.target_names = vocab.target_names\n\n        # optimizer\n        self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)\n\n        # count\n        self.step = 0\n        self.early_stop = -1\n        self.best_train_f1, self.best_dev_f1 = 0, 0\n        self.last_epoch = epochs\n\n    def train(self):\n        logging.info('Start training...')\n        for epoch in range(1, epochs + 1):\n            train_f1 = self._train(epoch)\n\n            dev_f1 = self._eval(epoch)\n\n            if self.best_dev_f1 <= dev_f1:\n                logging.info(\n                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n                torch.save(self.model.state_dict(), save_model)\n\n                self.best_train_f1 = train_f1\n                self.best_dev_f1 = dev_f1\n                self.early_stop = 0\n            else:\n                self.early_stop += 1\n                if self.early_stop == early_stops:\n                    logging.info(\n                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n                    self.last_epoch = epoch\n                    break\n    def test(self):\n        self.model.load_state_dict(torch.load(save_model))\n        self._eval(self.last_epoch + 1, test=True)\n\n    def _train(self, epoch):\n        self.optimizer.zero_grad()\n        self.model.train()\n\n        start_time = time.time()\n        epoch_start_time = time.time()\n        overall_losses = 0\n        losses = 0\n        batch_idx = 1\n        y_pred = []\n        y_true = []\n        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n            torch.cuda.empty_cache()\n            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n            batch_outputs = self.model(batch_inputs)\n            loss = self.criterion(batch_outputs, batch_labels)\n            loss.backward()\n\n            loss_value = loss.detach().cpu().item()\n            losses += loss_value\n            overall_losses += loss_value\n\n            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n            y_true.extend(batch_labels.cpu().numpy().tolist())\n\n            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n                optimizer.step()\n                scheduler.step()\n            self.optimizer.zero_grad()\n\n            self.step += 1\n\n            if batch_idx % log_interval == 0:\n                elapsed = time.time() - start_time\n\n                lrs = self.optimizer.get_lr()\n                logging.info(\n                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n                        epoch, self.step, batch_idx, self.batch_num, lrs,\n                        losses / log_interval,\n                        elapsed / log_interval))\n\n                losses = 0\n                start_time = time.time()\n\n            batch_idx += 1\n\n        overall_losses /= self.batch_num\n        during_time = time.time() - epoch_start_time\n\n        # reformat\n        overall_losses = reformat(overall_losses, 4)\n        score, f1 = get_score(y_true, y_pred)\n\n        logging.info(\n            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n                                                                                  overall_losses,\n                                                                                  during_time))\n        if set(y_true) == set(y_pred) and self.report:\n            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n            logging.info('\\n' + report)\n\n        return f1\n\n    def _eval(self, epoch, test=False):\n        self.model.eval()\n        start_time = time.time()\n        data = self.test_data if test else self.dev_data\n        y_pred = []\n        y_true = []\n        with torch.no_grad():\n            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n                torch.cuda.empty_cache()\n                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n                batch_outputs = self.model(batch_inputs)\n                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n                y_true.extend(batch_labels.cpu().numpy().tolist())\n\n            score, f1 = get_score(y_true, y_pred)\n\n            during_time = time.time() - start_time\n            \n            if test:\n                df = pd.DataFrame({'label': y_pred})\n                df.to_csv(save_test, index=False, sep=',')\n            else:\n                logging.info(\n                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n                                                                              during_time))\n                if set(y_true) == set(y_pred) and self.report:\n                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n                    logging.info('\\n' + report)\n\n        return f1\n\n    def batch2tensor(self, batch_data):\n        '''\n            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n        '''\n        batch_size = len(batch_data)\n        doc_labels = []\n        doc_lens = []\n        doc_max_sent_len = []\n        for doc_data in batch_data:\n            doc_labels.append(doc_data[0])\n            doc_lens.append(doc_data[1])\n            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n            max_sent_len = max(sent_lens)\n            doc_max_sent_len.append(max_sent_len)\n\n        max_doc_len = max(doc_lens)\n        max_sent_len = max(doc_max_sent_len)\n\n        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n        batch_labels = torch.LongTensor(doc_labels)\n\n        for b in range(batch_size):\n            for sent_idx in range(doc_lens[b]):\n                sent_data = batch_data[b][2][sent_idx]\n                for word_idx in range(sent_data[0]):\n                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n                    batch_masks[b, sent_idx, word_idx] = 1\n\n        if use_cuda:\n            batch_inputs1 = batch_inputs1.to(device)\n            batch_inputs2 = batch_inputs2.to(device)\n            batch_masks = batch_masks.to(device)\n            batch_labels = batch_labels.to(device)\n\n        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels","metadata":{},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# train\ntrainer = Trainer(model, vocab)\ntrainer.train()","metadata":{},"execution_count":12,"outputs":[{"name":"stderr","output_type":"stream","text":"2020-07-17 12:03:16,802 INFO: Total 9000 docs.\n\n2020-07-17 12:03:20,218 INFO: Total 1000 docs.\n\n2020-07-17 12:06:19,245 INFO: Total 50000 docs.\n\n2020-07-17 12:06:19,245 INFO: Start training...\n\n2020-07-17 12:06:38,493 INFO: | epoch   1 | step  50 | batch  50/563 | lr 0.00020 0.00005 | loss 2.0764 | s/batch 0.38\n\n2020-07-17 12:06:57,096 INFO: | epoch   1 | step 100 | batch 100/563 | lr 0.00020 0.00004 | loss 1.2976 | s/batch 0.37\n\n2020-07-17 12:07:14,320 INFO: | epoch   1 | step 150 | batch 150/563 | lr 0.00020 0.00004 | loss 0.8577 | s/batch 0.34\n\n2020-07-17 12:07:30,075 INFO: | epoch   1 | step 200 | batch 200/563 | lr 0.00020 0.00003 | loss 0.7951 | s/batch 0.32\n\n2020-07-17 12:07:48,252 INFO: | epoch   1 | step 250 | batch 250/563 | lr 0.00020 0.00003 | loss 0.7110 | s/batch 0.36\n\n2020-07-17 12:08:05,002 INFO: | epoch   1 | step 300 | batch 300/563 | lr 0.00020 0.00002 | loss 0.7157 | s/batch 0.33\n\n2020-07-17 12:08:23,702 INFO: | epoch   1 | step 350 | batch 350/563 | lr 0.00020 0.00002 | loss 0.4552 | s/batch 0.37\n\n2020-07-17 12:08:42,488 INFO: | epoch   1 | step 400 | batch 400/563 | lr 0.00020 0.00001 | loss 0.6583 | s/batch 0.38\n\n2020-07-17 12:08:59,988 INFO: | epoch   1 | step 450 | batch 450/563 | lr 0.00020 0.00001 | loss 0.4896 | s/batch 0.35\n\n2020-07-17 12:09:16,801 INFO: | epoch   1 | step 500 | batch 500/563 | lr 0.00020 0.00001 | loss 0.4260 | s/batch 0.34\n\n2020-07-17 12:09:35,899 INFO: | epoch   1 | step 550 | batch 550/563 | lr 0.00020 0.00000 | loss 0.4927 | s/batch 0.38\n\n2020-07-17 12:09:39,842 INFO: | epoch   1 | score (70.87, 58.89, 62.61) | f1 62.61 | loss 0.8048 | time 200.59\n\n2020-07-17 12:09:39,858 INFO: \n\n              precision    recall  f1-score   support\n\n\n\n          科技     0.7656    0.8179    0.7909      1697\n\n          股票     0.7021    0.8923    0.7858      1680\n\n          体育     0.8852    0.9224    0.9035      1405\n\n          娱乐     0.7780    0.8157    0.7964       971\n\n          时政     0.7809    0.7028    0.7398       710\n\n          社会     0.6862    0.7133    0.6995       558\n\n          教育     0.8396    0.7363    0.7845       455\n\n          财经     0.7212    0.3099    0.4335       384\n\n          家居     0.6667    0.6043    0.6339       374\n\n          游戏     0.7772    0.5125    0.6177       279\n\n          房产     0.7063    0.4633    0.5596       218\n\n          时尚     0.6747    0.3784    0.4848       148\n\n          彩票     0.9375    0.3750    0.5357        80\n\n          星座     0.0000    0.0000    0.0000        41\n\n\n\n    accuracy                         0.7647      9000\n\n   macro avg     0.7087    0.5889    0.6261      9000\n\nweighted avg     0.7632    0.7647    0.7544      9000\n\n\n\n/home/dell/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\n2020-07-17 12:09:55,088 INFO: | epoch   1 | dev | score (77.78, 73.19, 74.56) | f1 74.56 | time 15.23\n\n2020-07-17 12:09:55,089 INFO: Exceed history dev = 0.00, current dev = 74.56\n"}]},{"cell_type":"code","source":"# test\ntrainer.test()","metadata":{},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**关于Datawhale：**\n\n> Datawhale是一个专注于数据科学与AI领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。Datawhale 以“for the learner，和学习者一起成长”为愿景，鼓励真实地展现自我、开放包容、互信互助、敢于试错和勇于担当。同时 Datawhale 用开源的理念去探索开源内容、开源学习和开源方案，赋能人才培养，助力人才成长，建立起人与人，人与知识，人与企业和人与未来的联结。\n\n本次新闻文本分类学习，专题知识将在天池分享，详情可关注Datawhale：\n\n ![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279172547/1584432602983_kAxAvgQpG2.jpg)","metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":2}