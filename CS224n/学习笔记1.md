$$
\text { bank }=\left(\begin{array}{r}
0.286 \\
0.792 \\
-0.177 \\
-0.107 \\
0.109 \\
-0.549 \\
0.371
\end{array}\right)
$$

词向量**Word2vec**又称word embedding或者word representation,上面是分布式diostribution representation,不仅仅是一个简单的位置向量，每一个单词都有一个distribution representation,构成了向量空间。

**Word2vec**是一个词向量学习框架

**主要思想**：



- 有一大簇的文本
- 每一个单词在一个确定的词典中用以词向量表示
- Go through文本中的每一个位置，这个文本有一个中心单词 c 以及 context(“outside 单词”) o,观察这个单词周围的单词。
- 使用单词 c与单词 o的单词向量的相似性来计算给定的 概率
- 调整这个单词向量，直到最大化这个概率

![img](https://img2018.cnblogs.com/blog/1346871/201904/1346871-20190428090748482-2118879432.png)

我们需要计算的是 𝑃(𝑤𝑡+𝑗|𝑤𝑡)P(wt+j|wt)。

我们希望的是通过周围的单词预测出中间的单词 ‘into’ 。所以我们可以改变这些预测，如果我们从贝叶斯的角度看的话，那么就是我们先计算出当遇到 'into' 的时候，周围每个单词的概率作为先验，然后当遇到周围的单词的时候，用后验就可以算出单词是 ‘into’的概率。

**Word2vec的目标函数函数**

对于文本中的每一个位置，$t=1,.....,T$来预测用一个大小为m的窗口预测'context words',对于中心单词$w_j$，目标函数是
$$
L(\theta)=\prod_{t=1}^{T} \prod_{-m \leq 0 \atop j \neq 0} P\left(w_{t+j} \mid w_{t} ; \theta\right)
$$
*m* 表示窗口的大小，𝐿(𝜃)是关于最优化 𝜃的函数，那么 𝐽(𝜃)就是 negative log 似然函数。𝜃 恰恰就是单词向量

损失函数表示为:
$$
J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq S \leq m \atop j \neq 0} \log P\left(w_{t+j} \mid w_{t} ; \theta\right)
$$
上面的 𝐽(𝜃)是损失函数，也就是目标函数。最小化目标函数等价于最大化预测的准确率。

**P函数计算**

对于每个word，我们有两种表现方法，分别是：

- $v_w$ when 𝑤 is a center word
- $u_w$ when 𝑤 is a context word

那么对于中心单词 *c* 于 context word *o*来说：
$$
P(o \mid c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
$$
对于前面的例子来说就是：$P\left(u_{\text {problems }} \mid v_{\text {into }}\right)$ short for $\mathrm{P}\left(\text {problems} \mid \text {into} ; u_{\text {problems }}, v_{\text {into}}, \theta\right)$

上面公式的解释，我们使用的是$u_{o}^{T} v_{c}$来表示权重的大小。也就是两个单词间的关联程度，$u_{o}^{T} v_{c}$越大越相关，（从向量的角度解释）。分母是所有的情况。然后是一个 𝑆𝑜𝑓𝑡𝑚𝑎𝑥函数：
$$
\operatorname{softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j=1}^{n} \exp \left(x_{j}\right)}=p_{i}
$$
**梯度下降最小化损失函数**

我们的参数只有一个 𝜃，但是请记住 𝜃 包含中心向量与 context word向量，所以是两截长。这里我们只是用 𝜃 来模糊的表示位置参数，那么 𝜃 到底是一个什么样的参数呢？在 CBOW 与 skip-gram中可以理解为两个矩阵。这个后面再说。
$$
\theta=\left[\begin{array}{l}
v_{\text {aardvark }} \\
v_{a} \\
\vdots \\
v_{\text {zebra }} \\
u_{\text {aardvark }} \\
u_{a} \\
\vdots \\
u_{z e b r a}
\end{array}\right] \in \mathbb{R}^{2 d V}
$$
































