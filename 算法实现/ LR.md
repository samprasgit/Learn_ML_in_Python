### 1.为什么 logistic regression 的输入特征一般是离散的而不是连续的？

（1）离散特征的增加和减少都很容易，易于模型的快速迭代。 

（2）稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。

（3）对异常数据具有较强的鲁棒性。 

（4）单个特征离散化为 N 个后，每个特征有单独的权重，相当于引入了非线性，增加了模型的表达能力，加大了拟合能力。 

（5）可以特征交叉，M + N 个特征变为 M * N 个特征，进一步引入非线性，提升表达能力。 

（6）特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

### 2.xgboost怎么给特征评分？

在训练的过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。 特征评分可以看成是被用来分离决策树的次数。

提高机器学习模型性能的五个关键方法https://blog.csdn.net/ebzxw/article/details/82528059

3.数据不均衡处理

https://blog.csdn.net/u012879957/article/details/82459538

https://www.cnblogs.com/kamekin/p/9824294.html

随机采样

SMOTE    KNN()而改进的方法则是先根据规则判断出少数类的边界样本，再对这些样本生成新样本。

在数据平衡的分类问题中，分类器好坏的评估指标常用准确率，但是对于数据不平衡的分类问题，准确率不再是恰当的评估指标。所以针对不平衡数据分类问题，常用f1-score、ROC-AUC曲线。
f1-score计算公式：

5、惩罚算法
使用惩罚学习算法，提高少数类的分类错误的成本，在训练期间使用参数class_weight='balanced’来惩罚少数群体类的错误，其数量与其代表性不足的数量成正比。
6、使用基于树的算法
基于树的算法在数据不平衡数据集上表现良好，因为它们的层次结构允许它们从两个类中学习信号。

KNN基本思想： 在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类。

优缺点

**优点**

对异常值不敏感

无数据输入假定

简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；

**缺点**：计算量太大

可理解性差

样本不均衡

依赖训练数据

最大的缺点是无法给出数据的内在含义